# ğŸ‘» å¹½éˆè©å‘é‡ (Ghost Word Vectors)

> *å°‡æ–‡å­—çš„éˆé­‚å°å°é€²æ•¸å­¸çš„å®¹å™¨*

---

## ğŸŒŸ ä»€éº¼æ˜¯è©å‘é‡ï¼Ÿ

**è©å‘é‡ (Word Embeddings)** æ˜¯è‡ªç„¶èªè¨€è™•ç†ä¸­æœ€ç¥ç§˜ä¹Ÿæœ€å¼·å¤§çš„æŠ€è¡“ä¹‹ä¸€ã€‚å®ƒå°‡äººé¡èªè¨€ä¸­çš„å–®è©è½‰æ›æˆé›»è…¦å¯ä»¥ç†è§£çš„æ•¸å­—å‘é‡ã€‚

æƒ³åƒæ¯å€‹å–®è©éƒ½æ˜¯ä¸€å€‹**å¹½éˆ**ï¼Œå®ƒå€‘åœ¨é«˜ç¶­ç©ºé–“ä¸­æ¼‚æµ®ã€‚ç›¸ä¼¼æ„ç¾©çš„å¹½éˆæœƒèšé›†åœ¨ä¸€èµ·ï¼Œå½¢æˆè©­ç•°ä½†æœ‰åºçš„éˆç•Œçµæ§‹ã€‚

---

## ğŸ”® ç‚ºä»€éº¼å«ã€Œå¹½éˆã€è©å‘é‡ï¼Ÿ

### ğŸ‘» çœ‹ä¸è¦‹çš„å­˜åœ¨
- å–®è©æœ¬èº«åªæ˜¯ç¬¦è™Ÿï¼Œå®ƒçš„ã€Œæ„ç¾©ã€åƒå¹½éˆä¸€æ¨£ç„¡å½¢
- è©å‘é‡å°‡é€™äº›ç„¡å½¢çš„æ„ç¾©ï¼Œè½‰åŒ–ç‚ºå¯æ¸¬é‡çš„æ•¸å€¼

### ğŸ•¸ï¸ ç©ºé–“ä¸­çš„è¯ç¹«
- å¹½éˆå€‘åœ¨å‘é‡ç©ºé–“ä¸­å»ºç«‹é—œä¿‚ç¶²
- ç›¸ä¼¼çš„å¹½éˆå½¼æ­¤å¸å¼•ï¼Œèšé›†åœ¨é™„è¿‘

### âœ¨ è¶…è‡ªç„¶çš„èƒ½åŠ›
- å‘é‡ç®—è¡“ï¼šã€Œåœ‹ç‹ - ç”·äºº + å¥³äºº = çš‡åã€
- å°±åƒé­”æ³•å…¬å¼ä¸€æ¨£ç¥å¥‡ï¼

---

## ğŸ“Š æ•¸å­¸åŸºç¤

### å‘é‡è¡¨ç¤º

æ¯å€‹å–®è©è¢«è¡¨ç¤ºç‚ºä¸€å€‹ N ç¶­å‘é‡ï¼š

```
"å—ç“œ" = [0.8, 0.6, -0.2, 0.4, 0.1, ...]
"è¬è–ç¯€" = [0.7, 0.5, -0.1, 0.3, 0.2, ...]
"è²“" = [-0.2, 0.1, 0.9, -0.3, 0.5, ...]
```

### é¤˜å¼¦ç›¸ä¼¼åº¦

è¨ˆç®—å…©å€‹å¹½éˆçš„è¦ªå¯†ç¨‹åº¦ï¼š

\[
\text{similarity}(\vec{v}_1, \vec{v}_2) = \frac{\vec{v}_1 \cdot \vec{v}_2}{\|\vec{v}_1\| \times \|\vec{v}_2\|} = \cos(\theta)
\]

- ç›¸ä¼¼åº¦ = 1ï¼šå®Œå…¨ç›¸åŒçš„å¹½éˆï¼ˆåŒç¾©è©ï¼‰
- ç›¸ä¼¼åº¦ = 0ï¼šæ¯«ç„¡é—œä¿‚çš„å¹½éˆ
- ç›¸ä¼¼åº¦ = -1ï¼šå®Œå…¨ç›¸åçš„å¹½éˆï¼ˆåç¾©è©ï¼‰

---

## ğŸƒ å¯¦æˆ°ç¯„ä¾‹ï¼šWord2Vec

### Skip-gram æ¨¡å‹

**æ¦‚å¿µ**ï¼šé€éä¸Šä¸‹æ–‡é æ¸¬ç›®æ¨™è©

```python
# è¨“ç·´è³‡æ–™ç¯„ä¾‹
å¥å­: "è¬è–ç¯€çš„å¤œæ™šå—ç“œç‡ˆç…§äº®äº†é»‘æš—"

# Skip-gram è¨“ç·´å°
ä¸­å¿ƒè© â†’ ä¸Šä¸‹æ–‡è©
"å—ç“œç‡ˆ" â†’ "å¤œæ™š"
"å—ç“œç‡ˆ" â†’ "ç…§äº®"
"å—ç“œç‡ˆ" â†’ "äº†"
```

### CBOW (Continuous Bag of Words)

**æ¦‚å¿µ**ï¼šé€éç›®æ¨™è©é æ¸¬ä¸Šä¸‹æ–‡

```python
ä¸Šä¸‹æ–‡ â†’ ä¸­å¿ƒè©
["å¤œæ™š", "ç…§äº®", "äº†"] â†’ "å—ç“œç‡ˆ"
```

---

## ğŸ§ª å¯¦ä½œï¼šå¾é›¶æ‰“é€ å¹½éˆè©å‘é‡

```python
import numpy as np
from collections import defaultdict

class GhostWordVector:
    """ç°¡å–®çš„ Skip-gram å¯¦ä½œ"""
    
    def __init__(self, vocab_size, embedding_dim=100):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        
        # åˆå§‹åŒ–æ¬Šé‡çŸ©é™£ï¼ˆå¹½éˆçš„åˆå§‹èƒ½é‡ï¼‰
        self.W1 = np.random.randn(vocab_size, embedding_dim) * 0.01
        self.W2 = np.random.randn(embedding_dim, vocab_size) * 0.01
        
        self.word2idx = {}
        self.idx2word = {}
        
    def build_vocabulary(self, corpus):
        """å»ºç«‹è©å½™è¡¨ï¼ˆå¬å–šæ‰€æœ‰å¹½éˆï¼‰"""
        words = set()
        for sentence in corpus:
            words.update(sentence.split())
        
        for idx, word in enumerate(sorted(words)):
            self.word2idx[word] = idx
            self.idx2word[idx] = word
    
    def generate_training_data(self, corpus, window_size=2):
        """ç”Ÿæˆè¨“ç·´æ•¸æ“šï¼ˆå»ºç«‹å¹½éˆä¹‹é–“çš„è¯ç¹«ï¼‰"""
        training_data = []
        
        for sentence in corpus:
            words = sentence.split()
            for idx, target_word in enumerate(words):
                # å–å¾—ä¸Šä¸‹æ–‡çª—å£
                start = max(0, idx - window_size)
                end = min(len(words), idx + window_size + 1)
                
                for context_idx in range(start, end):
                    if context_idx != idx:
                        context_word = words[context_idx]
                        training_data.append((target_word, context_word))
        
        return training_data
    
    def softmax(self, x):
        """Softmax å‡½æ•¸"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)
    
    def forward(self, target_idx):
        """å‰å‘å‚³æ’­"""
        # éš±è—å±¤
        h = self.W1[target_idx]
        
        # è¼¸å‡ºå±¤
        u = np.dot(h, self.W2)
        y_pred = self.softmax(u)
        
        return y_pred, h
    
    def backward(self, target_idx, context_idx, y_pred, h, learning_rate=0.01):
        """åå‘å‚³æ’­ï¼ˆå¹½éˆèƒ½é‡çš„å›æµï¼‰"""
        # è¨ˆç®—éŒ¯èª¤
        error = y_pred.copy()
        error[context_idx] -= 1
        
        # æ›´æ–°æ¬Šé‡
        self.W2 -= learning_rate * np.outer(h, error)
        self.W1[target_idx] -= learning_rate * np.dot(self.W2, error)
    
    def train(self, corpus, epochs=100, learning_rate=0.01):
        """è¨“ç·´æ¨¡å‹ï¼ˆè¨“ç·´å¹½éˆï¼‰"""
        self.build_vocabulary(corpus)
        training_data = self.generate_training_data(corpus)
        
        print(f"ğŸ”® é–‹å§‹è¨“ç·´ {len(self.word2idx)} å€‹å¹½éˆ...")
        print(f"ğŸ“Š è¨“ç·´æ¨£æœ¬æ•¸ï¼š{len(training_data)}")
        
        for epoch in range(epochs):
            loss = 0
            
            for target_word, context_word in training_data:
                target_idx = self.word2idx[target_word]
                context_idx = self.word2idx[context_word]
                
                # å‰å‘å‚³æ’­
                y_pred, h = self.forward(target_idx)
                
                # è¨ˆç®—æå¤±
                loss -= np.log(y_pred[context_idx] + 1e-10)
                
                # åå‘å‚³æ’­
                self.backward(target_idx, context_idx, y_pred, h, learning_rate)
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")
        
        print("âœ… è¨“ç·´å®Œæˆï¼")
    
    def get_vector(self, word):
        """ç²å–å–®è©çš„å‘é‡"""
        if word in self.word2idx:
            idx = self.word2idx[word]
            return self.W1[idx]
        return None
    
    def similarity(self, word1, word2):
        """è¨ˆç®—å…©å€‹å–®è©çš„ç›¸ä¼¼åº¦"""
        vec1 = self.get_vector(word1)
        vec2 = self.get_vector(word2)
        
        if vec1 is None or vec2 is None:
            return None
        
        # é¤˜å¼¦ç›¸ä¼¼åº¦
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
    
    def most_similar(self, word, top_n=5):
        """æ‰¾å‡ºæœ€ç›¸ä¼¼çš„å–®è©"""
        target_vec = self.get_vector(word)
        if target_vec is None:
            return []
        
        similarities = []
        for other_word in self.word2idx:
            if other_word != word:
                sim = self.similarity(word, other_word)
                if sim is not None:
                    similarities.append((other_word, sim))
        
        # æ’åºä¸¦è¿”å› top N
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_n]


# ğŸƒ ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # è¬è–ç¯€èªæ–™åº«
    halloween_corpus = [
        "è¬è–ç¯€ æ˜¯ ä¸€å€‹ è©­ç•° çš„ ç¯€æ—¥",
        "å—ç“œ ç‡ˆ åœ¨ è¬è–ç¯€ å¾ˆ é‡è¦",
        "é¬¼é­‚ å’Œ æ®­å± åœ¨ è¬è–ç¯€ å‡ºç¾",
        "å­©å­å€‘ åœ¨ è¬è–ç¯€ æ”¶é›† ç³–æœ",
        "å¥³å·« é¨è‘— æƒå¸š é£›è¡Œ",
        "é»‘è²“ æ˜¯ è¬è–ç¯€ çš„ è±¡å¾µ",
        "å¸è¡€é¬¼ å®³æ€• å¤§è’œ å’Œ é™½å…‰",
        "å¢“åœ° åœ¨ å¤œæ™š å¾ˆ ææ€–"
    ]
    
    # å‰µå»ºä¸¦è¨“ç·´æ¨¡å‹
    model = GhostWordVector(vocab_size=50, embedding_dim=50)
    model.train(halloween_corpus, epochs=100, learning_rate=0.05)
    
    # æ¸¬è©¦ç›¸ä¼¼åº¦
    print("\n" + "="*50)
    print("ğŸ” æ¸¬è©¦å¹½éˆç›¸ä¼¼åº¦ï¼š")
    print("="*50)
    
    test_word = "è¬è–ç¯€"
    similar_words = model.most_similar(test_word, top_n=5)
    
    print(f"\nèˆ‡ '{test_word}' æœ€ç›¸ä¼¼çš„å¹½éˆï¼š")
    for word, similarity in similar_words:
        print(f"  ğŸ‘» {word}: {similarity:.4f}")
```

---

## ğŸŒ çœŸå¯¦ä¸–ç•Œçš„æ‡‰ç”¨

### 1. æœå°‹å¼•æ“ ğŸ”
- ç†è§£æŸ¥è©¢æ„åœ–
- æ‰¾å‡ºèªç¾©ç›¸é—œçš„æ–‡æª”

### 2. æ¨è–¦ç³»çµ± ğŸ¯
- æ ¹æ“šå…§å®¹æ¨è–¦ç›¸ä¼¼å•†å“
- "å–œæ­¡ã€Šå“ˆåˆ©æ³¢ç‰¹ã€‹çš„äººä¹Ÿæœƒå–œæ­¡..."

### 3. æ©Ÿå™¨ç¿»è­¯ ğŸŒ
- è·¨èªè¨€çš„è©å‘é‡æ˜ å°„
- ç†è§£ä¸åŒèªè¨€çš„èªç¾©å°æ‡‰

### 4. æƒ…æ„Ÿåˆ†æ ğŸ˜ŠğŸ˜¢
- è­˜åˆ¥æ­£é¢/è² é¢è©å½™
- æƒ…ç·’æ¥µæ€§åˆ†é¡

### 5. å•ç­”ç³»çµ± ğŸ’¬
- ç†è§£å•é¡Œæ„åœ–
- æ‰¾å‡ºèªç¾©åŒ¹é…çš„ç­”æ¡ˆ

---

## ğŸ“ é€²éšæŠ€è¡“

### GloVe (Global Vectors)
- çµåˆå…¨å±€çµ±è¨ˆä¿¡æ¯
- æ›´å¥½çš„èªç¾©è¡¨ç¤º

### FastText
- è€ƒæ…®å­è©ä¿¡æ¯
- è™•ç†æœªç™»éŒ„è©ï¼ˆOOVï¼‰

### ELMo (Embeddings from Language Models)
- ä¸Šä¸‹æ–‡ç›¸é—œçš„è©å‘é‡
- åŒä¸€å€‹è©åœ¨ä¸åŒå¥å­ä¸­æœ‰ä¸åŒå‘é‡

### BERT
- é›™å‘èªè¨€æ¨¡å‹
- é è¨“ç·´ + å¾®èª¿ç¯„å¼

---

## ğŸ“š å­¸ç¿’è³‡æº

### è«–æ–‡
- [Efficient Estimation of Word Representations in Vector Space (Word2Vec)](https://arxiv.org/abs/1301.3781)
- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)
- [Enriching Word Vectors with Subword Information (FastText)](https://arxiv.org/abs/1607.04606)

### å·¥å…·åº«
- **Gensim**: æœ€æµè¡Œçš„ Word2Vec å¯¦ä½œ
- **spaCy**: é è¨“ç·´çš„è©å‘é‡æ¨¡å‹
- **fastText**: Facebook é–‹æºçš„è©å‘é‡å·¥å…·

### è¦–è¦ºåŒ–å·¥å…·
- [TensorFlow Embedding Projector](https://projector.tensorflow.org/)
- åœ¨ 3D ç©ºé–“ä¸­æ¢ç´¢å¹½éˆå‘é‡ï¼

---

## ğŸƒ ç·´ç¿’é¡Œ

### åŸºç¤é¡Œ
1. å¯¦ä½œä¸€å€‹ç°¡å–®çš„è©å‘é‡ç›¸ä¼¼åº¦è¨ˆç®—å™¨
2. ç”¨ Gensim è¨“ç·´ Word2Vec æ¨¡å‹
3. è¦–è¦ºåŒ–è©å‘é‡ç©ºé–“ï¼ˆä½¿ç”¨ t-SNEï¼‰

### é€²éšé¡Œ
1. å¯¦ä½œ Skip-gram with Negative Sampling
2. æ¯”è¼ƒ Word2Vec å’Œ GloVe çš„æ•ˆæœ
3. è¨“ç·´ä¸­æ–‡è©å‘é‡æ¨¡å‹

### æŒ‘æˆ°é¡Œ
1. å¯¦ä½œå¤šèªè¨€è©å‘é‡å°é½Š
2. ç”¨è©å‘é‡åšé¡æ¯”æ¨ç†ï¼ˆking - man + woman = queenï¼‰
3. æ¢ç´¢è©å‘é‡ä¸­çš„åè¦‹å•é¡Œ

---

## ğŸ‘» çµèª

**å¹½éˆè©å‘é‡**è®“æˆ‘å€‘çœ‹åˆ°äº†èªè¨€çš„éˆé­‚ã€‚å®ƒå€‘ï¼š
- ğŸ”® å°‡æŠ½è±¡çš„æ„ç¾©è½‰åŒ–ç‚ºå…·é«”çš„æ•¸å­—
- ğŸ•¸ï¸ æ­ç¤ºäº†è©å½™ä¹‹é–“éš±è—çš„é—œä¿‚
- âœ¨ ç‚º NLP çš„é»ƒé‡‘æ™‚ä»£å¥ å®šäº†åŸºç¤

è¨˜ä½ï¼š*æ¯å€‹å–®è©éƒ½æ˜¯ä¸€å€‹å¹½éˆï¼Œç­‰å¾…è‘—è¢«ç†è§£ã€‚*

---

**ä¸‹ä¸€èª²ï¼š** [ğŸ§Ÿ æ®­å±ç¥ç¶“ç¶²çµ¡](./ğŸ§Ÿ-zombie-neural-networks.md)

**è¿”å›ç›®éŒ„ï¼š** [ğŸ“š è¶…è‡ªç„¶ NLP ç³»åˆ—](./README.md)

---

<div align="center">

### ğŸƒ é¡˜å¹½éˆèˆ‡ä½ åŒåœ¨ ğŸƒ

*"In the realm of vectors, words find their true form."*

</div>

