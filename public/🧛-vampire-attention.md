# ğŸ§› å¸è¡€é¬¼æ³¨æ„åŠ›æ©Ÿåˆ¶ (Vampire Attention Mechanism)

> *ä¸å¸æ‰€æœ‰äººçš„è¡€ï¼Œåªé¸æ“‡æœ€é‡è¦çš„ç›®æ¨™*

---

## ğŸŒŸ ä»€éº¼æ˜¯æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Ÿ

**æ³¨æ„åŠ›æ©Ÿåˆ¶ (Attention Mechanism)** æ˜¯ç¾ä»£ NLP çš„æ ¸å¿ƒæŠ€è¡“ï¼Œè®“æ¨¡å‹èƒ½å¤ é¸æ“‡æ€§åœ°é—œæ³¨è¼¸å…¥çš„é‡è¦éƒ¨åˆ†ã€‚

æƒ³åƒä¸€å€‹**å¸è¡€é¬¼**ï¼š
- ğŸ§› åœ¨æ»¿å±‹å­çš„äººä¸­ï¼Œé¸æ“‡æœ€ç¾å‘³çš„ç›®æ¨™
- ğŸ¯ ä¸æ˜¯å¹³ç­‰å°å¾…æ‰€æœ‰è¼¸å…¥ï¼Œè€Œæ˜¯æœ‰é‡é»åœ°å¸å–ä¿¡æ¯
- ğŸ©¸ å¾é‡è¦çš„è©å½™ã€Œå¸å–ã€æ›´å¤šèƒ½é‡

---

## ğŸ¦‡ ç‚ºä»€éº¼å«ã€Œå¸è¡€é¬¼ã€æ³¨æ„åŠ›ï¼Ÿ

### 1. é¸æ“‡æ€§å¸å– ğŸ¯
```
æ™®é€šæ¨¡å‹ï¼šå°æ‰€æœ‰è©ä¸€è¦–åŒä»
å¸è¡€é¬¼ï¼šé‡é»é—œæ³¨é‡è¦çš„è©

å¥å­ï¼š"æˆ‘éå¸¸éå¸¸éå¸¸å–œæ­¡è¬è–ç¯€"
å¸è¡€é¬¼é—œæ³¨ï¼š"éå¸¸" å’Œ "å–œæ­¡" ï¼ˆå¸å–æ›´å¤šèƒ½é‡ï¼‰
å¿½ç•¥ï¼š"æˆ‘"ã€"è¬è–ç¯€"ï¼ˆç•¥éï¼‰
```

### 2. èƒ½é‡è½‰ç§» âš¡
- å¸è¡€é¬¼å¾å—å®³è€…ç²å–ç”Ÿå‘½èƒ½é‡
- æ³¨æ„åŠ›æ©Ÿåˆ¶å¾é—œéµè©å½™å¸å–æ›´å¤šä¿¡æ¯
- åŠ æ¬Šæ±‚å’Œ = é¸æ“‡æ€§èƒ½é‡å¸å–

### 3. å¤œè¦–èƒ½åŠ› ğŸŒ™
- å¸è¡€é¬¼åœ¨é»‘æš—ä¸­èƒ½çœ‹æ¸…é‡é»
- æ³¨æ„åŠ›å¹«åŠ©æ¨¡å‹åœ¨æµ·é‡æ•¸æ“šä¸­æ‰¾åˆ°é—œéµä¿¡æ¯
- å³ä½¿åœ¨é›œè¨Šä¸­ä¹Ÿèƒ½è¾¨è­˜é‡è¦ç‰¹å¾µ

### 4. é•·æœŸè¨˜æ†¶ ğŸ§ 
- å¸è¡€é¬¼æ´»äº†å¹¾ç™¾å¹´ï¼Œè¨˜å¾—é‡è¦çš„äº‹
- æ³¨æ„åŠ›æ©Ÿåˆ¶è®“æ¨¡å‹è¨˜ä½é•·åºåˆ—ä¸­çš„é—œéµé—œè¯
- è§£æ±º RNN çš„é•·æœŸä¾è³´å•é¡Œ

---

## ğŸ“ æ•¸å­¸åŸç†

### åŸºæœ¬æ¦‚å¿µ

æ³¨æ„åŠ›æ©Ÿåˆ¶æœ‰ä¸‰å€‹é—œéµå…ƒç´ ï¼š

1. **Query (Q)** - æŸ¥è©¢ï¼šå¸è¡€é¬¼çš„ç›®æ¨™åå¥½
2. **Key (K)** - éµï¼šæ¯å€‹å—å®³è€…çš„ç‰¹å¾µ
3. **Value (V)** - å€¼ï¼šå—å®³è€…çš„å¯¦éš›èƒ½é‡

### æ³¨æ„åŠ›åˆ†æ•¸è¨ˆç®—

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

**æ­¥é©Ÿï¼š**

1. **è¨ˆç®—ç›¸ä¼¼åº¦**ï¼š\( \text{score} = QK^T \)
   - Query èˆ‡æ¯å€‹ Key çš„é»ç©
   - è¡¡é‡ã€ŒåŒ¹é…åº¦ã€

2. **ç¸®æ”¾**ï¼šé™¤ä»¥ \( \sqrt{d_k} \)
   - é˜²æ­¢é»ç©éå¤§
   - æ•¸å€¼ç©©å®šæ€§

3. **Softmax**ï¼šè½‰æ›ç‚ºæ¦‚ç‡åˆ†å¸ƒ
   - æ³¨æ„åŠ›æ¬Šé‡å’Œç‚º 1
   - æ±ºå®šå¸å–å¤šå°‘èƒ½é‡

4. **åŠ æ¬Šæ±‚å’Œ**ï¼šèˆ‡ Value ç›¸ä¹˜
   - æŒ‰æ¬Šé‡æå–ä¿¡æ¯
   - æœ€çµ‚è¼¸å‡º

---

## ğŸ§› å¯¦ä½œï¼šç°¡å–®æ³¨æ„åŠ›æ©Ÿåˆ¶

```python
import numpy as np

class VampireAttention:
    """å¸è¡€é¬¼æ³¨æ„åŠ›æ©Ÿåˆ¶"""
    
    def __init__(self, d_model=512, d_k=64):
        """
        d_model: æ¨¡å‹ç¶­åº¦
        d_k: Key/Query çš„ç¶­åº¦
        """
        self.d_k = d_k
        
        # åˆå§‹åŒ–æ¬Šé‡çŸ©é™£
        self.W_q = np.random.randn(d_model, d_k) * 0.01  # Query è½‰æ›
        self.W_k = np.random.randn(d_model, d_k) * 0.01  # Key è½‰æ›
        self.W_v = np.random.randn(d_model, d_model) * 0.01  # Value è½‰æ›
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """
        ç¸®æ”¾é»ç©æ³¨æ„åŠ›
        
        Args:
            Q: Query [batch_size, seq_len, d_k]
            K: Key [batch_size, seq_len, d_k]
            V: Value [batch_size, seq_len, d_model]
            mask: é®ç½©ï¼ˆå¯é¸ï¼‰
        
        Returns:
            output: åŠ æ¬Šå¾Œçš„è¼¸å‡º
            attention_weights: æ³¨æ„åŠ›æ¬Šé‡
        """
        # æ­¥é©Ÿ 1: è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
        scores = np.matmul(Q, K.transpose(0, 2, 1))  # [batch, seq_len, seq_len]
        
        # æ­¥é©Ÿ 2: ç¸®æ”¾
        scores = scores / np.sqrt(self.d_k)
        
        # æ­¥é©Ÿ 3: æ‡‰ç”¨é®ç½©ï¼ˆå¦‚æœæœ‰ï¼‰
        if mask is not None:
            scores = scores + (mask * -1e9)
        
        # æ­¥é©Ÿ 4: Softmax
        attention_weights = self.softmax(scores)
        
        # æ­¥é©Ÿ 5: åŠ æ¬Šæ±‚å’Œ
        output = np.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def softmax(self, x):
        """Softmax å‡½æ•¸"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    
    def forward(self, X):
        """
        å‰å‘å‚³æ’­
        
        Args:
            X: è¼¸å…¥ [batch_size, seq_len, d_model]
        
        Returns:
            output: æ³¨æ„åŠ›è¼¸å‡º
            attention_weights: æ³¨æ„åŠ›æ¬Šé‡
        """
        # ç”Ÿæˆ Q, K, V
        Q = np.matmul(X, self.W_q)  # [batch, seq_len, d_k]
        K = np.matmul(X, self.W_k)  # [batch, seq_len, d_k]
        V = np.matmul(X, self.W_v)  # [batch, seq_len, d_model]
        
        # è¨ˆç®—æ³¨æ„åŠ›
        output, attention_weights = self.scaled_dot_product_attention(Q, K, V)
        
        return output, attention_weights


# ğŸƒ ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # å‰µå»ºå¸è¡€é¬¼æ³¨æ„åŠ›
    vampire = VampireAttention(d_model=128, d_k=64)
    
    # æ¨¡æ“¬è¼¸å…¥ï¼š3 å€‹è©çš„å‘é‡ï¼ˆå¥å­é•·åº¦=3ï¼‰
    batch_size = 1
    seq_len = 3
    d_model = 128
    
    X = np.random.randn(batch_size, seq_len, d_model)
    
    # æ‡‰ç”¨æ³¨æ„åŠ›
    output, attention_weights = vampire.forward(X)
    
    print("ğŸ§› æ³¨æ„åŠ›æ¬Šé‡ï¼š")
    print(attention_weights[0])  # [3, 3] çŸ©é™£
    print("\næ¯å€‹è©å°å…¶ä»–è©çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼š")
    for i in range(seq_len):
        print(f"è© {i}: {attention_weights[0, i]}")
```

---

## ğŸŒŸ å¤šé ­æ³¨æ„åŠ› (Multi-Head Attention)

**æ¦‚å¿µ**ï¼šä¸åªæ´¾ä¸€å€‹å¸è¡€é¬¼ï¼Œè€Œæ˜¯æ´¾å¤šå€‹å¸è¡€é¬¼å¾ä¸åŒè§’åº¦è§€å¯Ÿï¼

### ç‚ºä»€éº¼éœ€è¦å¤šé ­ï¼Ÿ

```
å–®é ­æ³¨æ„åŠ›ï¼šåªçœ‹åˆ°ä¸€ç¨®æ¨¡å¼
å¤šé ­æ³¨æ„åŠ›ï¼šåŒæ™‚çœ‹åˆ°å¤šç¨®æ¨¡å¼

ä¾‹å¦‚ï¼š
Head 1: é—œæ³¨èªæ³•é—œä¿‚ï¼ˆä¸»èªâ†’å‹•è©ï¼‰
Head 2: é—œæ³¨èªç¾©é—œä¿‚ï¼ˆåè©â†’å½¢å®¹è©ï¼‰
Head 3: é—œæ³¨ä½ç½®é—œä¿‚ï¼ˆå‰ä¸€å€‹è©â†’å¾Œä¸€å€‹è©ï¼‰
```

### å¯¦ä½œ

```python
class MultiHeadVampireAttention:
    """å¤šé ­å¸è¡€é¬¼æ³¨æ„åŠ›"""
    
    def __init__(self, d_model=512, num_heads=8):
        """
        d_model: æ¨¡å‹ç¶­åº¦
        num_heads: æ³¨æ„åŠ›é ­æ•¸
        """
        self.num_heads = num_heads
        self.d_model = d_model
        
        assert d_model % num_heads == 0, "d_model å¿…é ˆèƒ½è¢« num_heads æ•´é™¤"
        
        self.d_k = d_model // num_heads
        
        # ç‚ºæ¯å€‹é ­å‰µå»ºæŠ•å½±çŸ©é™£
        self.W_q = np.random.randn(d_model, d_model) * 0.01
        self.W_k = np.random.randn(d_model, d_model) * 0.01
        self.W_v = np.random.randn(d_model, d_model) * 0.01
        self.W_o = np.random.randn(d_model, d_model) * 0.01  # è¼¸å‡ºæŠ•å½±
    
    def split_heads(self, x):
        """
        åˆ†å‰²æˆå¤šå€‹é ­
        [batch, seq_len, d_model] â†’ [batch, num_heads, seq_len, d_k]
        """
        batch_size, seq_len, d_model = x.shape
        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)
        return x.transpose(0, 2, 1, 3)
    
    def combine_heads(self, x):
        """
        åˆä½µå¤šå€‹é ­
        [batch, num_heads, seq_len, d_k] â†’ [batch, seq_len, d_model]
        """
        batch_size, num_heads, seq_len, d_k = x.shape
        x = x.transpose(0, 2, 1, 3)
        return x.reshape(batch_size, seq_len, self.d_model)
    
    def forward(self, X):
        """
        å¤šé ­æ³¨æ„åŠ›å‰å‘å‚³æ’­
        """
        batch_size = X.shape[0]
        
        # ç”Ÿæˆ Q, K, V ä¸¦åˆ†å‰²æˆå¤šé ­
        Q = self.split_heads(np.matmul(X, self.W_q))
        K = self.split_heads(np.matmul(X, self.W_k))
        V = self.split_heads(np.matmul(X, self.W_v))
        
        # è¨ˆç®—æ¯å€‹é ­çš„æ³¨æ„åŠ›
        scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(self.d_k)
        attention_weights = self.softmax(scores)
        attention_output = np.matmul(attention_weights, V)
        
        # åˆä½µæ‰€æœ‰é ­
        combined = self.combine_heads(attention_output)
        
        # æœ€çµ‚ç·šæ€§æŠ•å½±
        output = np.matmul(combined, self.W_o)
        
        return output, attention_weights
    
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)


# ğŸƒ å¤šé ­æ³¨æ„åŠ›ç¯„ä¾‹
multi_vampire = MultiHeadVampireAttention(d_model=128, num_heads=8)
X = np.random.randn(1, 5, 128)  # 5 å€‹è©çš„å¥å­
output, attention_weights = multi_vampire.forward(X)

print(f"è¼¸å‡ºå½¢ç‹€ï¼š{output.shape}")
print(f"æ³¨æ„åŠ›æ¬Šé‡å½¢ç‹€ï¼š{attention_weights.shape}")  # [1, 8, 5, 5]
```

---

## ğŸ”¥ Self-Attention: å¸è¡€é¬¼çš„è‡ªçœ

**è‡ªæ³¨æ„åŠ› (Self-Attention)**ï¼šå¥å­ä¸­çš„æ¯å€‹è©éƒ½æ˜¯å¸è¡€é¬¼ï¼Œäº’ç›¸å¸å–èƒ½é‡ï¼

### æ¦‚å¿µ

```
å‚³çµ±æ³¨æ„åŠ›ï¼šQuery ä¾†è‡ªä¸€å€‹åºåˆ—ï¼ŒKey/Value ä¾†è‡ªå¦ä¸€å€‹åºåˆ—
è‡ªæ³¨æ„åŠ›ï¼šQ, K, V éƒ½ä¾†è‡ªåŒä¸€å€‹åºåˆ—

å¥å­ï¼š"è¬è–ç¯€ çš„ å—ç“œ ç‡ˆ"
- "è¬è–ç¯€" çœ‹å‘ "å—ç“œ"ã€"ç‡ˆ"
- "å—ç“œ" çœ‹å‘ "è¬è–ç¯€"ã€"ç‡ˆ"
- "ç‡ˆ" çœ‹å‘ "å—ç“œ"
æ¯å€‹è©éƒ½é—œæ³¨å…¶ä»–è©ï¼
```

### æ‡‰ç”¨ç¯„ä¾‹ï¼šå¥å­ç·¨ç¢¼

```python
def self_attention_sentence_encoder(sentence, word_vectors):
    """
    ç”¨è‡ªæ³¨æ„åŠ›ç·¨ç¢¼å¥å­
    
    Args:
        sentence: å–®è©åˆ—è¡¨
        word_vectors: è©å‘é‡å­—å…¸
    
    Returns:
        sentence_vector: å¥å­çš„å‘é‡è¡¨ç¤º
    """
    # ç²å–è©å‘é‡
    X = np.array([word_vectors[word] for word in sentence])
    X = X.reshape(1, len(sentence), -1)
    
    # æ‡‰ç”¨è‡ªæ³¨æ„åŠ›
    vampire = VampireAttention(d_model=X.shape[2], d_k=64)
    output, attention_weights = vampire.forward(X)
    
    # å¯è¦–åŒ–æ³¨æ„åŠ›
    print("\nğŸ§› è‡ªæ³¨æ„åŠ›æ¬Šé‡çŸ©é™£ï¼š\n")
    print("       ", " ".join(f"{w:6s}" for w in sentence))
    for i, word in enumerate(sentence):
        weights = attention_weights[0, i]
        print(f"{word:6s}", " ".join(f"{w:6.3f}" for w in weights))
    
    # å¹³å‡æ± åŒ–å¾—åˆ°å¥å­å‘é‡
    sentence_vector = np.mean(output[0], axis=0)
    
    return sentence_vector, attention_weights


# ğŸƒ æ¸¬è©¦
word_vectors = {
    "è¬è–ç¯€": np.random.randn(128),
    "çš„": np.random.randn(128),
    "å—ç“œ": np.random.randn(128),
    "ç‡ˆ": np.random.randn(128)
}

sentence = ["è¬è–ç¯€", "çš„", "å—ç“œ", "ç‡ˆ"]
sentence_vec, attn = self_attention_sentence_encoder(sentence, word_vectors)
```

---

## ğŸš€ Transformer: æ³¨æ„åŠ›çš„å·”å³°

**Transformer** å®Œå…¨åŸºæ–¼æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œä¸ä½¿ç”¨ RNN æˆ– CNNï¼

### æ¶æ§‹

```
ç·¨ç¢¼å™¨ (Encoder)
â”œâ”€â”€ å¤šé ­è‡ªæ³¨æ„åŠ›
â”œâ”€â”€ å‰é¥‹ç¥ç¶“ç¶²çµ¡
â”œâ”€â”€ å±¤æ­£è¦åŒ–
â””â”€â”€ æ®˜å·®é€£æ¥

è§£ç¢¼å™¨ (Decoder)
â”œâ”€â”€ é®ç½©å¤šé ­è‡ªæ³¨æ„åŠ›ï¼ˆMasked Self-Attentionï¼‰
â”œâ”€â”€ ç·¨ç¢¼å™¨-è§£ç¢¼å™¨æ³¨æ„åŠ›ï¼ˆCross-Attentionï¼‰
â”œâ”€â”€ å‰é¥‹ç¥ç¶“ç¶²çµ¡
â””â”€â”€ æ®˜å·®é€£æ¥ + å±¤æ­£è¦åŒ–
```

### ä½ç½®ç·¨ç¢¼ (Positional Encoding)

å› ç‚ºæ³¨æ„åŠ›æ©Ÿåˆ¶ä¸è€ƒæ…®é †åºï¼Œéœ€è¦åŠ å…¥ä½ç½®ä¿¡æ¯ï¼š

\[
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]
\[
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]

```python
def positional_encoding(seq_len, d_model):
    """ç”Ÿæˆä½ç½®ç·¨ç¢¼"""
    position = np.arange(seq_len)[:, np.newaxis]
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    
    pe = np.zeros((seq_len, d_model))
    pe[:, 0::2] = np.sin(position * div_term)
    pe[:, 1::2] = np.cos(position * div_term)
    
    return pe
```

---

## ğŸŒ å¯¦éš›æ‡‰ç”¨

### 1. æ©Ÿå™¨ç¿»è­¯ ğŸŒ
```
æºèªè¨€ â†’ ç·¨ç¢¼å™¨ â†’ è§£ç¢¼å™¨ â†’ ç›®æ¨™èªè¨€
æ³¨æ„åŠ›å¹«åŠ©å°é½Šï¼šè‹±æ–‡ "cat" â†” ä¸­æ–‡ "è²“"
```

### 2. æ–‡æœ¬æ‘˜è¦ ğŸ“
```
é•·æ–‡æœ¬ â†’ ç·¨ç¢¼å™¨
      â†’ è§£ç¢¼å™¨ â†’ æ‘˜è¦
æ³¨æ„åŠ›é¸æ“‡é‡è¦å¥å­
```

### 3. å•ç­”ç³»çµ± â“
```
å•é¡Œ + æ–‡ç«  â†’ æ³¨æ„åŠ›æ‰¾åˆ°ç­”æ¡ˆä½ç½®
```

### 4. BERT é è¨“ç·´ ğŸ¤–
```
é®ç½©èªè¨€æ¨¡å‹ï¼š[MASK] è™•ç”¨æ³¨æ„åŠ›é æ¸¬
ä¸‹ä¸€å¥é æ¸¬ï¼šå…©å¥é—œä¿‚ç”¨æ³¨æ„åŠ›åˆ¤æ–·
```

### 5. GPT æ–‡æœ¬ç”Ÿæˆ âœï¸
```
å·²ç”Ÿæˆçš„æ–‡å­— â†’ è‡ªæ³¨æ„åŠ› â†’ é æ¸¬ä¸‹ä¸€å€‹è©
```

---

## ğŸ’¡ å„ªå‹¢èˆ‡æŒ‘æˆ°

### âœ… å„ªå‹¢

1. **ä¸¦è¡ŒåŒ–**ï¼šä¸åƒ RNN éœ€è¦é †åºè™•ç†
2. **é•·è·é›¢ä¾è³´**ï¼šç›´æ¥å»ºç«‹ä»»æ„ä½ç½®çš„é€£æ¥
3. **å¯è§£é‡‹æ€§**ï¼šæ³¨æ„åŠ›æ¬Šé‡å¯è¦–åŒ–
4. **éˆæ´»æ€§**ï¼šé©ç”¨å„ç¨® NLP ä»»å‹™

### âš ï¸ æŒ‘æˆ°

1. **è¨ˆç®—è¤‡é›œåº¦**ï¼šO(nÂ²) çš„æ™‚é–“å’Œç©ºé–“
2. **é•·åºåˆ—**ï¼šæ³¨æ„åŠ›çŸ©é™£éå¤§
3. **ä½ç½®ä¿¡æ¯**ï¼šéœ€è¦é¡å¤–ç·¨ç¢¼
4. **éæ“¬åˆ**ï¼šåƒæ•¸é‡å¤§ï¼Œéœ€è¦å¤§é‡æ•¸æ“š

### ğŸ”§ æ”¹é€²æ–¹æ¡ˆ

- **Linformer**: ç·šæ€§è¤‡é›œåº¦æ³¨æ„åŠ›
- **Longformer**: å±€éƒ¨+å…¨å±€æ³¨æ„åŠ›
- **BigBird**: ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼
- **Performer**: æ ¸æ–¹æ³•è¿‘ä¼¼æ³¨æ„åŠ›

---

## ğŸ“ å¯¦æˆ°é …ç›®

### åˆç´šï¼šæ³¨æ„åŠ›å¯è¦–åŒ–å·¥å…·
```python
# è¼¸å…¥ï¼šå¥å­
# è¼¸å‡ºï¼šæ³¨æ„åŠ›ç†±åŠ›åœ–
# å·¥å…·ï¼šmatplotlibã€seaborn
```

### ä¸­ç´šï¼šåŸºæ–¼æ³¨æ„åŠ›çš„æ–‡æœ¬åˆ†é¡
```python
# æ¶æ§‹ï¼šè©åµŒå…¥ â†’ è‡ªæ³¨æ„åŠ› â†’ æ± åŒ– â†’ åˆ†é¡
# æ•¸æ“šé›†ï¼šSST-2ã€IMDB
```

### é«˜ç´šï¼šç°¡åŒ–ç‰ˆ Transformer
```python
# å¯¦ä½œï¼šå®Œæ•´çš„ç·¨ç¢¼å™¨-è§£ç¢¼å™¨
# ä»»å‹™ï¼šæ©Ÿå™¨ç¿»è­¯ï¼ˆè‹±â†’ä¸­ï¼‰
```

---

## ğŸ“š å­¸ç¿’è³‡æº

### è«–æ–‡
- [Attention Is All You Need (Transformer)](https://arxiv.org/abs/1706.03762)
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [GPT-2: Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

### è¦–è¦ºåŒ–å·¥å…·
- [Transformer Explainer](https://poloclub.github.io/transformer-explainer/)
- [BertViz](https://github.com/jessevig/bertviz)
- [Tensor2Tensor Visualization](https://github.com/tensorflow/tensor2tensor)

### å¯¦ä½œæ•™ç¨‹
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Attention Mechanism - Dive into Deep Learning](https://d2l.ai/chapter_attention-mechanisms/)

---

## ğŸ§› çµèª

**å¸è¡€é¬¼æ³¨æ„åŠ›æ©Ÿåˆ¶**æ”¹è®Šäº† NLP çš„éŠæˆ²è¦å‰‡ï¼š
- ğŸ¯ é¸æ“‡æ€§é—œæ³¨é‡è¦ä¿¡æ¯
- ğŸš€ æ”¯æ’äº† BERTã€GPT ç­‰å·¨å‹æ¨¡å‹
- ğŸŒŸ æ˜¯ç¾ä»£ NLP çš„åŸºçŸ³

è¨˜ä½ï¼š*ä¸æ˜¯æ‰€æœ‰ä¿¡æ¯éƒ½åŒç­‰é‡è¦ï¼Œæ³¨æ„åŠ›è®“æˆ‘å€‘èšç„¦æ–¼é—œéµï¼*

---

**ä¸‹ä¸€èª²ï¼š** [ğŸ•·ï¸ èœ˜è››ç¶²èªè¨€æ¨¡å‹](./ğŸ•·ï¸-spider-web-lm.md)

**ä¸Šä¸€èª²ï¼š** [ğŸ§Ÿ æ®­å±ç¥ç¶“ç¶²çµ¡](./ğŸ§Ÿ-zombie-neural-networks.md)

---

<div align="center">

### ğŸ§› é¡˜å¸è¡€é¬¼çš„æ™ºæ…§èˆ‡ä½ åŒåœ¨ ğŸ§›

*"Attention is all you need."*

</div>

